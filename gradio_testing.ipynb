{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6bf3d30-8116-4e9e-9591-46db08e6eb8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Are you wearing a mask?\n",
    "import gradio as gr\n",
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "# Face masks\n",
    "# TODO: Allow user selectable model?\n",
    "\n",
    "models = [\"model_weights/face_masks_v8.pt\",\"model_weights/face_masks_partial.pt\"]\n",
    "\n",
    "def yolo(im, model, size=640):\n",
    "    g = (size / max(im.size))  # gain\n",
    "    im = im.resize((int(x * g) for x in im.size), Image.ANTIALIAS)  # resize\n",
    "    \n",
    "    model = torch.hub.load('ultralytics/yolov5', 'custom', model)\n",
    "\n",
    "    results = model(im)  # inference\n",
    "    results.render()  # updates results.imgs with boxes and labels\n",
    "    return Image.fromarray(results.imgs[0])\n",
    "\n",
    "image = gr.inputs.Image(type='pil', label=\"Original Image\")\n",
    "model = gr.inputs.Dropdown(choices = models, type = 'value',label=\"Model Weight\")\n",
    "\n",
    "inputs = [image,model]\n",
    "\n",
    "outputs = gr.outputs.Image(type=\"pil\", label=\"Output Image\")\n",
    "\n",
    "title = \"Are you wearing a mask?\"\n",
    "description = \"Detecting masked and unmasked faces with YOLOv5. Take a picture, upload an image, or click an example image to use.\"\n",
    "article = \"<p style='text-align: center'>This app makes predictions using a YOLOv5s model that was fine tuned on a <a href='https://www.kaggle.com/datasets/henrylydecker/face-masks'>dataset</a> of people with and without masks. All of the code  for training the model is available on <a href='https://github.com/hlydecker/are-you-wearing-a-mask'>GitHub</a>. This app and the model behind it were created by Henry Lydecker, for a <a href='https://github.com/Sydney-Informatics-Hub/cv-demo'>course</a> he developed for the Sydney Informatics Hub, a Core Research Facility of The University of Sydney. Find out more about the YOLO model from the original creator, <a href='https://pjreddie.com/darknet/yolo/'>Joseph Redmon</a>. YOLOv5 is a family of compound-scaled object detection models trained on the COCO dataset and developed by Ultralytics, and includes simple functionality for Test Time Augmentation (TTA), model ensembling, hyperparameter evolution, and export to ONNX, CoreML and TFLite. <a href='https://github.com/ultralytics/yolov5'>Source code</a> | <a href='https://pytorch.org/hub/ultralytics_yolov5'>PyTorch Hub</a></p>\"\n",
    "\n",
    "examples = [['data/picard.jpg'], ['data/crowd.jpeg'],['data/baseball2.jpeg'],['data/santa-claus-orig.jpg'],['data/kfc_anime2.jpg'],['data/doge2.webp'],['data/cat_mask.jpg']]\n",
    "gr.Interface(yolo, inputs, outputs, title=title, description=description, article=article, examples=examples, theme=\"huggingface\").launch(enable_queue=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "09ed6bf3-11a6-4e14-b335-4e32aeab6bb3",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "non-default argument follows default argument (2366577109.py, line 19)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Input \u001b[0;32mIn [6]\u001b[0;36m\u001b[0m\n\u001b[0;31m    def yolo(img_input, size=640, model_name):\u001b[0m\n\u001b[0m                                            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m non-default argument follows default argument\n"
     ]
    }
   ],
   "source": [
    "# MegaDetector v5 Demo\n",
    "import gradio as gr\n",
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "# Markdown Content\n",
    "title = \"\"\"<h1 id=\"title\">MegaDetector v5</h1>\"\"\"\n",
    "description = \"Detect and identify animals, people and vehicles in camera trap images.\"\n",
    "article = \"<p style='text-align: center'>MegaDetector makes predictions using a YOLOv5 model that was trained to detect animals, humans, and vehicles in camera trap images; find out more about the project on <a href='https://github.com/microsoft/CameraTraps'>Microsoft's CameraTraps GitHub</a>. This app was built by <a href='https://github.com/hlydecker'>Henry Lydecker</a> but really depends on code and models developed by <a href='http://ecologize.org/'>Ecologize</a> and <a href='http://aka.ms/aiforearth'>Microsoft AI for Earth</a>. Find out more about the YOLO model from the original creator, <a href='https://pjreddie.com/darknet/yolo/'>Joseph Redmon</a>. YOLOv5 is a family of compound-scaled object detection models trained on the COCO dataset and developed by Ultralytics, and includes simple functionality for Test Time Augmentation (TTA), model ensembling, hyperparameter evolution, and export to ONNX, CoreML and TFLite. <a href='https://github.com/ultralytics/yolov5'>Source code</a> | <a href='https://pytorch.org/hub/ultralytics_yolov5'>PyTorch Hub</a></p>\"\n",
    "\n",
    "# Load MegaDetector v5a model\n",
    "# TODO: Allow user selectable model?\n",
    "models = [\"model_weights/face_masks_v8.pt\",\"model_weights/face_masks_partial.pt\"]\n",
    "\n",
    "# model = torch.hub.load('ultralytics/yolov5', 'custom', \"model_weights/md_v5a.0.0.pt\")\n",
    "\n",
    "def yolo(img_input, size=640, model_name):\n",
    "\n",
    "    model = model = torch.hub.load('ultralytics/yolov5', 'custom', model_name)\n",
    "    g = (size / max(im.size))  # gain\n",
    "    im = im.resize((int(x * g) for x in im.size), Image.ANTIALIAS)  # resize\n",
    "\n",
    "    results = model(im)  # inference\n",
    "    results.render()  # updates results.imgs with boxes and labels\n",
    "    return Image.fromarray(results.imgs[0])\n",
    "\n",
    "demo = gr.Blocks()\n",
    "\n",
    "with demo:\n",
    "    gr.Markdown(title)\n",
    "    gr.Markdown(description)\n",
    "    options = gr.Dropdown(choices=models, label=\"Select MegaDetector Model\", show_label=True)\n",
    "    \n",
    "    with gr.Row():\n",
    "        img_input = gr.Image(type='pil', label=\"Original Image\")\n",
    "        img_output = gr.Image(type=\"pil\", label=\"Output Image\")\n",
    "    \n",
    "    with gr.Row():\n",
    "        example_images = gr.Dataset(components=[img_input],\n",
    "                                    samples = [['data/picard.jpg'], ['data/crowd.jpeg'],['data/baseball2.jpeg'],['data/santa-claus-orig.jpg'],['data/kfc_anime2.jpg'],['data/doge2.webp'],['data/cat_mask.jpg']])\n",
    "    \n",
    "    detect_button = gr.Button('Detect')\n",
    "    \n",
    "    detect_button.click(yolo, inputs=[options,img_input], outputs=img_output, queue=True)\n",
    "    example_images.click(fn=set_example_image, inputs = [example_images], outputs=[img_input])\n",
    "    \n",
    "    gr.Markdown(article)\n",
    " \n",
    "demo.launch(enable_queue=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "10b43ec5-c68a-4f5f-9463-b2d5ae8835c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/henrylydecker/.cache/torch/hub/ultralytics_yolov5_master\n",
      "YOLOv5 ðŸš€ 2022-5-11 torch 1.10.2 CPU\n",
      "\n",
      "Fusing layers... \n",
      "Model summary: 213 layers, 1761871 parameters, 0 gradients, 4.2 GFLOPs\n",
      "Adding AutoShape... \n",
      "Using cache found in /Users/henrylydecker/.cache/torch/hub/ultralytics_yolov5_master\n",
      "YOLOv5 ðŸš€ 2022-5-11 torch 1.10.2 CPU\n",
      "\n",
      "Fusing layers... \n",
      "Model summary: 213 layers, 7015519 parameters, 0 gradients, 15.8 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7862/\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7862/\" width=\"900\" height=\"500\" allow=\"autoplay; camera; microphone;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(<gradio.routes.App at 0x7fbdf96b9310>, 'http://127.0.0.1:7862/', None)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/henrylydecker/opt/anaconda3/envs/cputest/lib/python3.9/site-packages/gradio/routes.py\", line 255, in run_predict\n",
      "    output = await app.blocks.process_api(\n",
      "  File \"/Users/henrylydecker/opt/anaconda3/envs/cputest/lib/python3.9/site-packages/gradio/blocks.py\", line 546, in process_api\n",
      "    predictions, duration = await self.call_function(fn_index, processed_input)\n",
      "  File \"/Users/henrylydecker/opt/anaconda3/envs/cputest/lib/python3.9/site-packages/gradio/blocks.py\", line 461, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(\n",
      "  File \"/Users/henrylydecker/opt/anaconda3/envs/cputest/lib/python3.9/site-packages/anyio/to_thread.py\", line 28, in run_sync\n",
      "    return await get_asynclib().run_sync_in_worker_thread(func, *args, cancellable=cancellable,\n",
      "  File \"/Users/henrylydecker/opt/anaconda3/envs/cputest/lib/python3.9/site-packages/anyio/_backends/_asyncio.py\", line 818, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"/Users/henrylydecker/opt/anaconda3/envs/cputest/lib/python3.9/site-packages/anyio/_backends/_asyncio.py\", line 754, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"/Users/henrylydecker/opt/anaconda3/envs/cputest/lib/python3.9/site-packages/gradio/interface.py\", line 508, in <lambda>\n",
      "    lambda *args: self.run_prediction(args)[0]\n",
      "  File \"/Users/henrylydecker/opt/anaconda3/envs/cputest/lib/python3.9/site-packages/gradio/interface.py\", line 712, in run_prediction\n",
      "    prediction = predict_fn(*processed_input)\n",
      "  File \"/var/folders/31/cjbmpgfs2fs0xpnc5z3hxjpw0000gp/T/ipykernel_85700/532902855.py\", line 13, in yolo\n",
      "    g = (size / max(im.size))  # gain\n",
      "AttributeError: 'NoneType' object has no attribute 'size'\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/henrylydecker/opt/anaconda3/envs/cputest/lib/python3.9/site-packages/gradio/routes.py\", line 255, in run_predict\n",
      "    output = await app.blocks.process_api(\n",
      "  File \"/Users/henrylydecker/opt/anaconda3/envs/cputest/lib/python3.9/site-packages/gradio/blocks.py\", line 546, in process_api\n",
      "    predictions, duration = await self.call_function(fn_index, processed_input)\n",
      "  File \"/Users/henrylydecker/opt/anaconda3/envs/cputest/lib/python3.9/site-packages/gradio/blocks.py\", line 461, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(\n",
      "  File \"/Users/henrylydecker/opt/anaconda3/envs/cputest/lib/python3.9/site-packages/anyio/to_thread.py\", line 28, in run_sync\n",
      "    return await get_asynclib().run_sync_in_worker_thread(func, *args, cancellable=cancellable,\n",
      "  File \"/Users/henrylydecker/opt/anaconda3/envs/cputest/lib/python3.9/site-packages/anyio/_backends/_asyncio.py\", line 818, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"/Users/henrylydecker/opt/anaconda3/envs/cputest/lib/python3.9/site-packages/anyio/_backends/_asyncio.py\", line 754, in run\n",
      "    result = context.run(func, *args)\n",
      "TypeError: 'NoneType' object is not callable\n"
     ]
    }
   ],
   "source": [
    "# MegaDetector v5 Demo\n",
    "import gradio as gr\n",
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "# Markdown Content\n",
    "title = \"\"\"<h1 id=\"title\">MegaDetector v5</h1>\"\"\"\n",
    "description = \"Detect and identify animals, people and vehicles in camera trap images.\"\n",
    "article = \"<p style='text-align: center'>MegaDetector makes predictions using a YOLOv5 model that was trained to detect animals, humans, and vehicles in camera trap images; find out more about the project on <a href='https://github.com/microsoft/CameraTraps'>Microsoft's CameraTraps GitHub</a>. This app was built by <a href='https://github.com/hlydecker'>Henry Lydecker</a> but really depends on code and models developed by <a href='http://ecologize.org/'>Ecologize</a> and <a href='http://aka.ms/aiforearth'>Microsoft AI for Earth</a>. Find out more about the YOLO model from the original creator, <a href='https://pjreddie.com/darknet/yolo/'>Joseph Redmon</a>. YOLOv5 is a family of compound-scaled object detection models trained on the COCO dataset and developed by Ultralytics, and includes simple functionality for Test Time Augmentation (TTA), model ensembling, hyperparameter evolution, and export to ONNX, CoreML and TFLite. <a href='https://github.com/ultralytics/yolov5'>Source code</a> | <a href='https://pytorch.org/hub/ultralytics_yolov5'>PyTorch Hub</a></p>\"\n",
    "\n",
    "# Load MegaDetector v5a model\n",
    "# TODO: Allow user selectable model?\n",
    "\n",
    "worst_model = torch.hub.load('ultralytics/yolov5', 'custom', \"model_weights/face_masks_partial.pt\")\n",
    "best_model = torch.hub.load('ultralytics/yolov5', 'custom', \"model_weights/face_masks_v8.pt\")\n",
    "\n",
    "examples = [['data/picard.jpg'],['data/crowd.jpeg'],['data/baseball2.jpeg'],['data/santa-claus-orig.jpg'],['data/kfc_anime2.jpg'],['data/doge2.webp'],['data/cat_mask.jpg']]\n",
    "\n",
    "def worst_yolo(im, size=640):\n",
    "\n",
    "    g = (size / max(im.size))  # gain\n",
    "    im = im.resize((int(x * g) for x in im.size), Image.ANTIALIAS)  # resize\n",
    "\n",
    "    results = worst_model(im)  # inference\n",
    "    results.render()  # updates results.imgs with boxes and labels\n",
    "    return Image.fromarray(results.imgs[0])\n",
    "\n",
    "def best_yolo(im, size=640):\n",
    "\n",
    "    g = (size / max(im.size))  # gain\n",
    "    im = im.resize((int(x * g) for x in im.size), Image.ANTIALIAS)  # resize\n",
    "\n",
    "    results = best_model(im)  # inference\n",
    "    results.render()  # updates results.imgs with boxes and labels\n",
    "    return Image.fromarray(results.imgs[0])\n",
    "\n",
    "def set_example_image(example: list) -> dict:\n",
    "    return gr.Image.update(value=example[0])\n",
    "\n",
    "demo = gr.Blocks()\n",
    "\n",
    "with demo:\n",
    "    gr.Markdown(title)\n",
    "    gr.Markdown(description)\n",
    "    \n",
    "    with gr.Tabs():\n",
    "        with gr.TabItem('Best Model'):\n",
    "            with gr.Row():\n",
    "                img_input = gr.Image(type='pil', label=\"Original Image\")\n",
    "                img_output_from_best= gr.Image(type=\"pil\", label=\"Output Image\")\n",
    "                \n",
    "            with gr.Row(): \n",
    "                example_images = gr.Dataset(components=[img_input],\n",
    "                                            samples=examples)\n",
    "                \n",
    "            best_but = gr.Button('Detect')\n",
    "     \n",
    "        with gr.TabItem('Worst Model'):\n",
    "            with gr.Row():\n",
    "                img_input = gr.Image(type='pil', label=\"Original Image\")\n",
    "                img_output_from_worst= gr.Image(type=\"pil\", label=\"Output Image\")\n",
    "                \n",
    "            with gr.Row(): \n",
    "                example_images = gr.Dataset(components=[img_input],\n",
    "                                            samples = examples)\n",
    "                \n",
    "            worst_but = gr.Button('Detect')\n",
    "        \n",
    "    \n",
    "    best_but.click(best_yolo,inputs=img_input,outputs=img_output_from_best,queue=True)\n",
    "    worst_but.click(worst_yolo,inputs=img_input,outputs=img_output_from_worst,queue=True)\n",
    "    example_images.click(fn=set_example_image,inputs=[example_images],outputs=[img_input])\n",
    "    \n",
    "demo.launch(enable_queue=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "048c55c5-539d-4469-b192-9e78629dc55c",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'gradio' has no attribute 'inputs'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [8]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m inputs \u001b[38;5;241m=\u001b[39m \u001b[43mgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[38;5;241m.\u001b[39mImage(\u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpil\u001b[39m\u001b[38;5;124m'\u001b[39m, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOriginal Image\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'gradio' has no attribute 'inputs'"
     ]
    }
   ],
   "source": [
    "inputs = gr.inputs.Image(type='pil', label=\"Original Image\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
